# Densely Connected Convolutional Networks

> alleviate:缓和，减轻
>
> nominal：名义上
>
> distill：蒸馏，提取
>
> summation：求和，和
>
> resurgence：再起，复活，复苏，复兴  
>
> revive：苏醒，复活
>
> bypass：绕过
>
> orthogonal：正交的，垂直的
>
> comprise:包含
>
> impede：阻碍，妨碍
>
> compactness：紧凑，简洁
>
> surrogate：替代
>
> whilst：同时，同while



## 0. Abstract

之前研究发现在输入层和输出层之前增加短连接，可以使神经网络变得很深，更加准确，训练更加有效。这篇文章提出了Dense Convolutional Network，简称DenseNet，以前向传播方式连接每一层和其他网络层。传统L层的网络有L层连接，前后紧邻的网络层之间一个连接，而DenseNet有$\frac{L\times (L+1)}{2}$层直接连接。

每一层以前面层处理的特征图作为输入，其自己的特征图作为后面网络层的输入。

 DenseNets  have  several  compelling  advantages:  

+ they  alleviate  the  vanishing-gradient problem
+ strengthen  feature  propagation
+ encourage  feature reuse
+ substantially reduce the number of parameters.



## 1. Introduction

> ResNets  and Highway Networks  by-pass signal from one layer to the next via identity connections.
>
> they all share a key characteristic:  **they create short paths from early layers to later layers.**

a simple **connectivity pattern**:  

to ensure maximum information flow between layers in the network, we **connect all  layers (with  matching  feature-map  sizes)  directly with each other**. To preserve the feed-forward nature,**each layer obtains additional inputs from all preceding layers  and  passes  on  its  own  feature-maps  to  all  subsequent layers**. 

这里可能反直觉的是**密集连接的模块比传统卷积网络需要更少的参数，因为它们不需要重新学习冗余的特征图**。

传统前向传播架构可以看作是带有状态的算法，1该状态在网络层之间传递。每一个网络层从前面的网络层读取状态，并写到后面的网络层。这改变了状态但也传递了需要保留的信息。

DenseNet进使用了很小的特征图集合来”收集知识“，保持剩下的特征图不变，最终的分类器根据网络的所有特征图来进行决策。

除了更好的参数效率，DenseNet的另一大优点是它们**提高了网络中信息和梯度的流动，使得训练变得容易**。

> Besides better parameter efficiency, one big advantage of DenseNets is their improved flow of information and gradients throughout the network, which makes them easy to train. 

每一层能够直接获取损失函数的梯度和最初的输入信号，结果是隐式深度监督。

> Each layer has direct access to the gradients from the loss function and the original input signal, leading to an **implicit deep supervision**.

Further, we also observe that **dense connections have a regularizing effect**, which reduces over-fitting on tasks with smaller training set sizes.



## 2. Related Work

Highway Networks几乎是第一个提供了端到端的有效训练方式的架构，可以训练超过100层的网络，使用了跳跃路径与门控单元，使得几百层的网络也能够容易优化。跳跃路径被认为是使得非常深的网络易于训练的关键因素。这一点在ResNet中得到证实。

最近，随机深度(stochastic depth)被提出成功训练1202层的ResNet，stochastic depth通过在训练中随机丢弃一些网络层来提高训练，这表明**不是所有的网络层都被需要，并强调了在深度(残差)网络中有大量冗余。**

另一中使得网络变深的方法是增加网络的宽度，GoogLeNet使用了Inception结构，连接了不同尺寸卷积核产生的特征图。**如果深度足够的话，增大ResNet每一层的卷积核数量能够提高表现。**FractalNet也能够通过使用宽的网络结构在一些数据集上获得较好的结果。

DenseNet通过特征复用，产生易于训练的压缩模型和极高的参数有效性，展现了网络的能力。

> DenseNets exploit the potential of the network through feature reuse, yielding condensed models that are easy to train and highly parameter-efficient.

连接不同网络层学习的特征图增加了后面网络层输入的多样性，提高了有效性。

> Concatenating feature-maps learned by different layers increases variation in the input of subsequent layers and improves efficiency.



## 3. DenseNets

1. ResNet的残差和shortcut是直接相加的，这回阻碍网络的信息流动。

2. **密集连接**。每一层以之前所有层的特征图作为输入，公式：
   $$
   \mathrm{x_\ell = H_\ell([x_0,x_1,...,x_{\ell-1}])}
   $$
   这里$\mathrm{[x_0,x_1,...x_{\ell-1}]}$表示这些特征图的连接。

3. **组合函数(Composite function)**。定义$H_\ell(.)$为三个连续操作的组合函数：BN, ReLU, Conv (3 * 3)。

4. 池化层。

   > We refer to layers between blocks as transition layers,  which do convolution and pooling.   The transition layers used in our experiments consist of **a batch normalization layer** and **an 1×1 convolutional layer** followed by **a 2×2 average pooling layer**.

5. 增长速率(Growth rate)。如果每个$H_\ell$产生k个特征图，那么$\ell^{th}$层有$k_0+k\times(\ell-1)$层输入特征图，k是网络的增长速率。和其他网络不同的是，**DenseNet可以有非常窄的网络层**。即使很小的增长速率也足够获得一些数据集上的最好表现。

   一个解释是，因为每一层能够获得之前的特征图，即网络的收集知识(collective knowledge)。我们可以将这些特征图看作网络的整体状态。每个网络层在这个状态上增加自己的k个特征图。增长速率规定了每层对全局状态的贡献。**全局状态，一旦写完成，就可以在网络任意一个地方访问，不需要传统网络那样在层与层之间复制**。

6. 瓶颈层(Bottleneck layers)。在$3\times3$网络层前引入$1\times1$网络层，减少特征图数量，提高计算有效性。

   >  In our experiments, we let each 1×1 convolution produce 4k feature-maps.

7. 压缩。To  further  improve  model  compactness, we  can  reduce  the  number  of  feature-maps  at  transition layers.

   如果一个dense block有m个特征图，我们设后面的transition层产生$\lfloor \theta m\rfloor$输出特征图，$0<\theta<1$，这里$\theta=0.5$，称$\theta<1$的模型为DenseNet-BC。

8. 实现细节。

   + DenseNet有3个dense block，每个有相同数量的网络层。

   + 在进入地一个dense block之前，执行16个卷积核的卷积(DenseNet-BC的话是32个)。

   + 每个$3\times3$卷积层，padding=1，0填充。

   + 两个dense block之间使用$1\times1$卷积接$2\times2$平均池化层作为transition层。

   + 在最后一个dense block的结尾，执行一个全局平均池化，然后加上一个softmax分类器。

     > At the end of the last dense block, a global average pooling is performed and then a softmax classifier is attached.  

   + 三个dense block的特征图大小$32\times32,16\times16,8\times8$。

   Basic DenseNet的设置：$\{L=40,k=12\},\{L=100,k=12\},\{L=100,k=24\}$。

   DenseNet-BC的设置：$\{L=100,k=12\},\{L=250,k=24\},\{L=190,k=40\}$。

   以上是除了ImageNet的数据集的设置。

   在ImageNet上的实验，使用DenseNet-BC，带有4个dense block，输入$3\times 224 \times 224$。最初的卷积层产生$2k$个卷积核，大小$7\times7$，步长为2。



## 4. Experiments

### (1) Datasets

介绍了CIFAR，SVHN，ImageNet数据集的一些信息，还有使用的数据增强方法，验证方法。

### (2) Training

优化器：SGD

在CIFAR，SVHN上：

+ batch size： 64
+ epoch： 340
+ 学习率：0.1，在训练epoch的50%和75%分别除以10。

在ImageNet上：

+ batch size：256
+ epoch：90
+ 学习率：0.1，在epoch 30, 60时除以10。

weight decay $10^{-4}$，momentum 0.9。

Kaiming初始化。

对于没有使用数据增强的数据集，C10,C100，SVHN，除了第一层卷积层，其他所有卷积层后增加Dropout层，dropout rate为0.2。

### (3) Classification Results on CIFAR and SVHN



### (4) Classification Results on ImageNet



## 5. Discussion



## 6. Conclusion

