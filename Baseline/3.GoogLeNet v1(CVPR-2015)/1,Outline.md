# Going Deeper with Convolutions

## 一、Abstract

我们提出了一个深度卷积神经网络结构叫做Inception，该结构获得了在图像分类和目标检测领域的最好表现。

该结构的主要标志是对网络中计算资源利用的提高。通过仔细设计，我们增加了网络的深度和宽度，同时保持了计算费用不变。为了优化质量，该结构的决策基于Hebbian原则和多尺度处理的直觉。



## 二、Introduction

现在计算机视觉的目标分类和检测的进步主要靠的是新思想、算法和网络结构。

我们在ILSVRC 2014上提交的GoogleNet的参数量是AlexNet的$1/12$，而且准确率更高。

关于目标检测，最大的收获不是简单地来自越来越大越深的网络，而是深度结构和经典计算机视觉的协同合作，例如R-CNN。

另一个需要注意的因素：随着移动和嵌入式计算的普及，算法的效率，尤其是功耗和使用的内存，越来越重要。这也是GoogleNet考虑的一大因素。

在大多数实验中，模型被设计成在推理时保留计算代价在1.5 billion multiply-adda，这样不止在学术领域，这模型也可以被用到现实世界，即使在更大的数据集上，也可以有合理的代价。

这篇论文主要讲用于计算机视觉的深度神经网络架构Inception，该名字从Network in Network(NIN)和“we need to go deeper”两篇论文中获得。这里的“deep”有的含义：

+ 我们通过Inception模块的形式引入架构中新的网络层，直接意义上就是增加的网络深度。

+ > 这个我也不知道怎么翻译。。。



## 三、Related Work

从LeNet-5开始CNN成为有一个标准的结构：堆叠的卷积层(后面可以选择加正则化或者最大池化层)，之后跟着一个或多个全连接层。这些基础设计的变种在图像分类领域很流行并且产生了到目前为止在MINST，CIFAR，IMageNet上的最好表现。最近的趋势是增加网络层数和宽度，同时使用dropout去处理过拟合的问题。

尽管最大池化曾会导致准确的空间信息的丢失，和AlexNet具有相似结构的卷积网络成功应用到目标定位、检测，人脸识别等领域。

受到视觉皮层(visual cortex)的神经元模型启发，Serre使用了一系列修改过的不同大小的Gabor filters来处理不同的尺度。我们使用了相似的策略。但是，Inception中的所有filter都是可学习的。此外，Inception层会重复很多次，其结果就是22层深的GoogLeNet模型。

Network-in-Network 是由 Lin提出的用来解决神经网络表示能力的一个方法，在他们的模型中，网络中增加了额外的$1\times1$的卷积层，增加了其深度。我们的架构中也深度使用了这个方法。但是，在我们的设置中，$1\times1$的卷基层有对偶目的：最重要的是，它们主要被用做维度归约模块(dimension reduction module)，用移除会限制网络大小的计算瓶颈。这允许我们能够继续增加网络深度和宽度而没有很大的性能损失。

最后，现在目标检测领域最好的算法是R-CNN。R-CNN将整体的目标检测问题分解成两个子问题：

+ 使用低级线索，比如色彩、纹理，以与类别无关的方式来产生目标位置候选区域。
+ 然后使用CNN分类器来标识那些位置的目标的类别。

这种两阶段的方法使用了带有低级线索bounding box的准确率，和CNN的高级分类能力。

在我们的目标检测提交中，使用了类似的pipeline，但是在两个阶段都进行提升，比如multi-box prediction for higher object bounding box recall and  ensemble  approaches  for  better categorization of bounding box proposals。



## 四、Motivation and High Level Considerations

最直接的提高深度神经网络的方法是增加其大小：

+ 增加网络深度。
+ 增加网络宽度，每层的神经元个数。

这是训练高质量模型的一个简单且安全的方法，尤其是给定许多标记好的训练数据的容量的时候，但是，这种简单的解决方法会带来两个问题。

+ 更大的网络大小意味着更多的参数，这使得扩大的网络更容易过拟合，尤其是训练数据集中的标记数据数量有限。这是标记数据集在花费巨大代价之后获得的主要瓶颈，这通常需要专业的人评分，来区分许多细粒度的视觉类别。
+ 增加网络大小的另一个缺点是会急剧增大对计算资源的使用。举个例子，在深度视觉网络中，如果两个卷积层连接，任何在卷积核数量上的统一增加会导致计算量的平方增加。如果增加的容量没有充分利用，例如多数权重最终接近于0,那么许多计算量被浪费了。因为计算资源总是有限的，一个高效的计算资源分布比随意的增加网络大小更好，即使当主要目标是提高表现的质量。

用于解决这两个问题的基础方法是引入稀疏性并使用稀疏的网络层代替全连接层，即使是在卷积层中。除了模仿生物系统，这也应当有来自Arora的突破性工作作为坚实的理论基础的好处。他们的主要结果表明如果数据集的概率分布能够被一个庞大的且稀疏的深的神经网络表示，那么最优的网络拓扑结构可以通过分析前面网络的激活函数和聚集和输出高度相关的神经元来一层一层地构造。尽管严格的数学证明需要非常强的条件，但是事实上这个理论和Hebbian准则协调得很好：连接在一起的神经元表明隐藏的idea是可应用的，即使在不那么严格的条件下。

不幸的是，当涉及到在非标准化的稀疏数据结构的数值计算，现在的计算设备还是不那么高效的。

即使代数运算操作减少了100倍，但是查找和缓存未命中的开销会占据主要地位，因此切换到稀疏矩阵不是那么合算的。这个差距会通过使用对密集矩阵计算进行高度优化的数值计算库而增大。并且，非标准化的稀疏矩阵仅仅是通过使用卷积来使用空间上的稀疏性。但是，卷积被实现为与较早层中补丁的密集连接的集合。

从AlexNet开始，卷积网络就在特征维度上传统地使用了随机和稀疏连接表，用于打破对称性并改善学习效果，然而趋势是变回全连接网络，为了更好的优化并行计算。

现在计算机视觉的最好的架构有标准化结构。越来越多的卷积核数量和更大的batch size使得密集计算得到充分利用。

这引起了关于是否有下一步、中间的步骤的问题：一个使用filter-level稀疏性的架构，正如理论表示的，是通过使用在稠密矩阵上的计算来表现我们当前的硬件。大量的稀疏矩阵计算表明了将稀疏矩阵聚集到相对稠密的子矩阵，用来给稀疏矩阵乘法提供可竞争的表现。看起来，在不久的将来会采用类似的方法来自动构建统一的深度学习架构。

Inception架构最初是作为一个案例来评估一个复杂的网络拓扑构造算法的假设输出，该算法试图逼近视觉网络的稀疏结构，并通过密集的，易于获得的组建来覆盖假设的结果。

景观这一开始是猜测的，但是和基于NIN的引用网络相比，在一开始还是获得了不错的提升。

经过一些调整，差距进一步扩大。Inception在定位和目标检测的背景下被证明特别有用。

有趣的是，尽管大多数原始的架构受到质疑，并在分类中进行彻底的测试，事实证明他们与局部最佳选择非常接近。

我们需要注意：尽管Inception结构在计算机视觉是成功的，但是还是有关于是够可以将这加入到指导准则上的疑问。确保这会需要更多的整体分析和证明。



## 五、Architectural Details



## 六、GoogleNet



## 七、Training Methodology




## 八、ILSVRC 2014 Classification Challenge Setup and Results



## 九、ILSVRC 2014 Detection Challenge Setup and Results



## 十、Conclusion



> hallmark：标志，标记
>
> intutition：直觉
>
> synergy：协同作用
>
> on the  ... font：关于
>
> traction：拖
>
> culmination：高潮
>
> prevalent：流行
>
> variant：变种
>
> prone：易于，倾向于
>
> fine-grained ：细颗粒的
>
> quadratic：二次方的
>
> indiscriminate：随意的，肆意的
>
> sparsity：稀疏性
>
> mimick：模仿
>
> groundbreaking：突破性的
>
> underpinning：坚实的基础
>
> symertry：对称性
>
> speculate：推测的，猜测的

