# 关于训练部分的小技巧

训练部分，会涉及到 **数据集、模型、损失函数、优化器**等部分，首先给出这些部分的超参数。

## 一、数据集

Dataset和DataLoader，配合一些超参数就行，不需要单独定义个函数。



## 二、模型

虽然自定义的模型会有一部分和官方的模型结构不一样，但是我们可以**部分加载这些网络层参数，达到迁移学习的效果**，而不用花费多余的时间学习。

可以通过**`state_dict()`方法获得模型的所有网络层参数**，返回的是**字典**，**键为网络层相关的名称(字符串)，值为网络层参数**。

通过这个方法获得官方模型和自定义模型的state_dict，然后将官方模型中和自定义模型相同键的部分的值赋值给自定义模型的state_dict，之后自定义模型加载修改过的state_dict就行。这个过程可以抽象成一个函数。

```python
def get_model():
    model = resnet50()
    resnet = models.resnet50(pretrained=True)
    resnet_state_dict = resnet.state_dict()
    model_state_dict = model.state_dict()
    for key in resnet_state_dict.keys():
        if key in model_state_dict.keys():
            model_state_dict[key] = resnet_state_dict[key]
    model.load_state_dict(model_state_dict)
    model.to(DEVICE)
    return model
```



## 三、损失函数

也没啥，直接定义就行。



## 四、优化器

优化器是训练的又一个重要部分，**优化器在训练过程中对模型参数进行更新**。因此，优化器一定需要模型的参数。

通过`name_parameters()`方法获得模型的参数，通过dict转成字典，**构造要传给优化器的参数以及每个参数的学习率、动量、衰减系数等**。

**传给优化器的是列表，列表中每个元素是字典，字典中是优化器的参数和对应的学习率等参数，这个params列表就是optimizer的params_group**。

```python
params = list()
params_dict = dict(model.named_parameters())
for key, value in params_dict.items():
    params.append({"params" : [value], "lr" : LEARNING_RATE})
optimizer = optim.Adam(params=params, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
```



这样，我们可以通过`params_group`获得构造出来的优化器的参数以及相关参数的信息，进而可以在训练过程中调整学习率相关数据。



## 五、训练过程

训练过程由多个epoch组成，每个epoch都可以抽象成一个函数`train_fn`，一次epoch涉及到的有 **数据集、模型、损失函数、优化器等**，通过将一次epoch封装为函数，方便理解和调试。

```python
def train_fn(epoch, train_loader, model, optimizer, loss_fn):
    """
    一个完整的训练EPOCH， 涉及到的有DataLoader， Model，Optimizer，Loss Function， 
    """
    model.train()
    mean_loss = list()
    for batch_idx, (inputs, targets) in enumerate(train_loader):
        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = loss_fn(outputs, targets)
        print(f"Epoch {epoch + 1}, iteration : {batch_idx + 1}, loss = {loss}")
        mean_loss.append(loss)
        loss.backward()
        optimizer.step()
    print(f"Epoch {epoch + 1} : Mean Loss was {(sum(mean_loss) / len(mean_loss)) :.4f}")
```

