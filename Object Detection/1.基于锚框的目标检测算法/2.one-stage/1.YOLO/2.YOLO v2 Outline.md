# YOLO9000: Better, Faster, Stronger

> novel: 新颖的
>
> centroid：形心，矩心
>
> harness：利用   leverage：利用
>
> scale to: 扩展到
>
> coherent：合乎逻辑的，有条理的
>
> synset：同义词集合



## 0. Abstract

提出了对YOLO的改进算法YOLO v2， YOLO9000是指用ImageNet， COCO联合训练。



## 1. Introduction

目标检测算法的数据集还是太小，因为目标检测数据集标注比分类困难，这里使用一种方法利用了分类数据集来扩充目标检测系统。



## 2. Better

YOLO和anchor based方法相比，在定位误差和召回率上较差，因此YOLO v2就这两方面进行了提升。

使用的技术：

+ **Batch Normalization**。

+ **High Resolution Classifier**。

+ **Convolutional With Anchor Box**。

  > 之前YOLO直接使用全连接层预测边框的坐标，而Faster R-CNN使用手工挑选的优先级来预测边框，仅仅在RPN中使用卷积层，Faster R-CNN预测anchor box的偏移和置信度。
  >
  > **预测偏移而不是坐标能够简化问题，使得网络更容易学习。**
  >
  > Predicting offsets instead of coordinates simplifies the problem and makes it easier for the network to learn.

  移除了全连接层，并使用anchor box来预测边框。去除了一个池化层使网络卷积层的输出有更高的分辨率。同时将网络处理的输入图片大小从448变到416,为了最终获得奇数长度边，这样中心就一个，最终$13\times13$。

  当使用anchor box时，我们需要将类型预测机制从空间定位中解耦，而是对每个anchor box预测类型和是否存在目标，预测也是需要计算IOU的，类型预测也是要用到条件概率的(存在目标的条件下的)。

  使用anchor box，虽然mAP略降，但是召回率提高很多。

+ **维度聚类**。使用K means clustering代替手动挑选优先级。如果使用欧几里德距离，那么大边框会产生比小边框多的错误，但是我们真正要的是**带来高IOU分数的优先级独立于边框的大小，使用的距离测量标准**：
  $$
  d(\mathrm{box,centroid})=1-\mathrm{IOU(box,centroid)}
  $$
  跑K mean clustering时，经过实验发现k = 5时较为合适，取得了模型复杂度和召回率的一个权衡。

  聚类形心和手动挑选anchor box有很大不同，短而宽的box少了，细长的box多了。

+ **Direct location prediction**。使用anchor box的YOLO会遇到**模型不稳定**的问题，尤其是在早期迭代中。多数不稳定性来自于对边框位置$(x,y)$的预测。在**RPN**中网络预测$t_x$和$t_y$，**中心坐标$(x,y)$的计算公式**如下：
  $$
  x=(t_x*w_a)-x_a\\
  y=(t_y*h_a)-y_a
  $$
  $w_a,h_a$是anchor box的宽度和高度。计算公式没有限制，所以任何anchor box可以在图片的任意点停止。使用随机初始化，模型用了很长时间到稳定来预测合理的偏移。

  没有使用偏移量预测，**YOLO v2直接预测和方格单元相关的位置坐标**。就是YOLO v1中用的归一化操作。

  网络中每个方格单元预测5个bounding box，每个边框产生5个坐标$t_x,t_y,t_w,t_h,t_o$。如果**方格单元相对图片左上角的偏移**是$(c_x,c_y)$，**bounding box的先验宽度、高度**$p_w,p_h$，那么预测就相当于:
  $$
  \begin{matrix}
  &b_x=\sigma(t_x)+c_x\\
  &b_y=\sigma(t_y)+c_y\\
  &b_w=p_we^{t_w}\\
  &b_h=p_he^{t_h}\\
  &Pr(object)*IOU(b,object)=\sigma(t_o)\\
  \end{matrix}
  $$
  因为我们**限制了位置预测，参数更容易学习，使得网络更稳定**。

  >  Using dimension clusters along with directly predicting the bounding box center location improves YOLO by almost 5% over the version with anchor boxes.

+ **细粒度特征(Fine-Grained Features)**。解决YOLO对小目标检测的准确率，从更高分辨率的层加一个跳跃层(类似ResNet的恒等映射)。

  跳跃层通过堆叠相邻特征到不同的通道而不是空间位置，将高分辨率特征和低分辨率特征拼接。这将$26\times26\times512$的特征图转换成$13\times13\times2048$的特征图，能够和原来的输出拼接。

+ **多尺度训练**。VGG采用的数据增强技巧，这里的尺寸范围为$[320,352,...608]$，每10个Epoch选择一个新的尺度。



## 3. Faster

VGG参数太多，GoogLeNet v1参数少，单模型准确率略低。这里提出了自己的darknet结构(现在基本上都是ResNet，即使现在ResNet性能都很好)。



## 4. Stronger

YOLO v2：

+ **标注了检测信息的图片**学习bounding box坐标预测，目标存在性以及如何分类普通目标。
+ **仅包含类型标签的图片**来扩充它可以检测的类型。

训练时：

+ **标注了检测信息的图片**：反向传播基于完整的YOLO v2损失函数。
+ **仅包含类型标签的分类图片**：仅从基于结构的分类部分的损失函数反向传播。

这种方式有几个问题：

+ 检测数据集只有普通的目标和标签，比如“狗”，“猫”。

+ 分类数据集的标签范围更广更深。

+ 如果我们要在这两个数据集上训练，我们就需要一个合理的方式来合并这些标签。 

  > If  we  want  to  train  on both datasets we need a coherent way to merge these labels.

+ 多数分类方法使用一个softmax层对所有可能的类型计算最终的概率分布。**使用softmax就假定了类型之间是互斥的**。这在合并数据集时会出问题。**我们使用多标签模型解决了这个问题，这种方法忽略了所有我们知道的关于数据的结构**。



**层次化分类(Hierarchical classification)**：多数分类方法假定了平的标签，但是对于合并的数据集，需要的是层次化的分类标签。虽然WordNet是有向图，这里使用的是从ImageNet构建的层次化树。

> 建立WordTree的细节：
>
> + 检查ImageNet中的视觉名词，查找它们在WordNet到根节点的路径，记为“实际对象”。
> + 许多同义词集合在图中只有一条路径，所以我们首先将这些路径加入树中。
> + 然后迭代检查剩下的概念，加入这些路径，然后尽可能小地增大树。
> + 如果一个概念有两条路径到根节点，选择短的那条。

**使用Word Tree进行分类，根据给定的同义集预测该同义集下每个意义的条件概率。**如果要计算特定节点的绝对概率，从该节点到根节点的路径上的条件概率乘起来就行。





## 5. Conclusion

YOLO v2提供了SOTA，并且比其他检测系统在多数目标检测数据集上更快的表现，并且可以在多种图片尺寸上运行来提供一个平滑的速度与准确率的权衡。

YOLO9000是一个实时的、能够检测超过9000类的目标监测系统，使用了WordTree合并不同数据集数据。

>  YOLO9000 is a strong step towards closing the dataset size gap between detection and classification.

Dataset combination using **hierarchical classification would be  useful  in  the  classification  and  segmentation  domains**.

> 未来的工作方向：弱监督图像分割。

